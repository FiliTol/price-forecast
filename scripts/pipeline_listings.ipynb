{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-29T15:51:45.304126Z",
     "start_time": "2024-07-29T15:51:43.470698Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "\n",
    "pd.options.display.float_format = '{:.0f}'.format\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:51:45.591452Z",
     "start_time": "2024-07-29T15:51:45.307762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_listings = pd.read_csv(\"../data/2023dic/d_listings.csv\")\n",
    "#df_listings = df_listings[['id', 'neighborhood_overview', 'host_id', 'host_since', 'host_location', 'host_about',\n",
    "#                           'host_response_time', 'host_response_rate', 'host_acceptance_rate', 'host_is_superhost',\n",
    "#                           'host_listings_count', 'host_total_listings_count', 'host_verifications',\n",
    "#                           'host_has_profile_pic', 'host_identity_verified', 'neighbourhood_cleansed',\n",
    "#                           'latitude', 'longitude', 'room_type',\n",
    "#                           'accommodates', 'bathrooms_text', 'beds', 'price',\n",
    "#                           'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'has_availability', 'availability_30',\n",
    "#                           'availability_60', 'availability_90', 'availability_365', 'number_of_reviews', 'first_review',\n",
    "#                           'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness',\n",
    "#                           'review_scores_checkin', 'review_scores_communication', 'review_scores_location',\n",
    "#                           'review_scores_value', 'calculated_host_listings_count',\n",
    "#                           'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms',\n",
    "#                           'calculated_host_listings_count_shared_rooms', 'reviews_per_month']]"
   ],
   "id": "6cf180bc5b7ac523",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:51:45.609400Z",
     "start_time": "2024-07-29T15:51:45.594267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_listings.drop(labels=[\"id\", \"listing_url\", \"name\", \"scrape_id\", \"last_scraped\", \"source\", \"description\", \"picture_url\", \"host_url\",\n",
    "                         \"host_name\", \"host_thumbnail_url\", \"host_picture_url\", \"host_neighbourhood\", \"neighbourhood\",\n",
    "                         \"neighbourhood_group_cleansed\", \"property_type\", \"amenities\", \"minimum_minimum_nights\",\n",
    "                         \"maximum_minimum_nights\", \"minimum_maximum_nights\", \"maximum_maximum_nights\", \"minimum_nights_avg_ntm\",\n",
    "                         \"maximum_nights_avg_ntm\", \"calendar_updated\", \"calendar_last_scraped\", \"number_of_reviews_ltm\",\n",
    "                         \"number_of_reviews_l30d\", \"license\", \"instant_bookable\"],\n",
    "                 axis=1,\n",
    "                 inplace=True)"
   ],
   "id": "d7b761af79c6b9f4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:51:46.301707Z",
     "start_time": "2024-07-29T15:51:46.278670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Drop rows with NaN in target \n",
    "df_listings = df_listings.loc[df_listings['price'].notnull(), :]\n",
    "df_listings.price.isnull().sum()"
   ],
   "id": "3f9f153a37911bc7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:51:48.520251Z",
     "start_time": "2024-07-29T15:51:48.511315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = df_listings.drop([\"price\"], axis=1, inplace=False)\n",
    "y = df_listings[\"price\"]"
   ],
   "id": "91ca6cb5d048b822",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:51:49.229352Z",
     "start_time": "2024-07-29T15:51:49.213193Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=874631)",
   "id": "a716932ae498c46",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Feature engineering\n",
    "#- `first_review` to `last_review` as date span\n",
    "#- `host_listings_count` as a % of `host_total_listings_count`\n",
    "#- Manage `neighbourhoods_cleansed` as a OHE of most frequent categories\n",
    "#- Distance between host home and listing location\n",
    "#- Distance between listing and relevant locations in town\n",
    "#- `host_since` encoded as *days of activity until period end (end of dataset scraping)*\n",
    "#- Sentiment of `neighborhood_overview` (investigate best sentiment technique for descriptions of appartments)\n",
    "#- Sentiment of `host_about` (investigate best sentiment technique for description of people)\n",
    "#- `host_id` and `id` as categorial\n",
    "#- `host_response_time` as ordinal variable\n",
    "#- string manipulation for `host_response_rate` and `host_acceptance_rate`\n",
    "#- `host_is_superhost` as binary categorial\n",
    "#- `host_verifications` as encoded in previous script\n",
    "#- `host_has_profile_pic` as binary\n",
    "#- `host_identity_verified` as binary\n",
    "#- keep `room_type` instead of `property_type` and make `room_type` a categorial with OHE\n",
    "#- `accomodates` used with `baths`, `beds` to compute the rate of beds and bathrooms for every person\n",
    "#- `price` with string manipulation\n",
    "#- `minimum_nighs_avg_ntm` as float\n",
    "#- `maximum_nights_avg_ntm` as float\n",
    "#- `has_availability` as binary\n",
    "#- all the `has_availability_NUMBER` as a % of the NUMBER of the feature\n",
    "#- `number_of_reviews` as an integer\n",
    "#- `review_scores_rating` as float\n",
    "#- all the reviews scores as float\n",
    "#- remove `calculated_host_listings_count` and keep the other three BUT **set them as % of the total host listings**\n",
    "#- `reviews_per_month` as float\n",
    "#- `longitude` and `latitude` standardization (because the values are both negatives and positives)"
   ],
   "id": "a72b15ace5d81379",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Transform feature datatypes\n",
    "\n",
    "In this section we execute the feature engineering without dealing with null values.\n",
    "We do it because once the types are cleaned, we want to plot a bit the data and explore it to see what is going on\n",
    "with NAs, frequency distribution, numeric distributions etc.\n",
    "In order to do so, we need to:\n",
    "1. Generalise the pipeline, because we would like to apply this script also to other similar dataset\n",
    "2. Return exceptions for NAs, to carry them on to the data exploration section\n",
    "\n",
    "> ***NOTE*** that the `feature-engine` library enables us to split the dataset into train and test just after the data type and feature engineering. This because the library contains some functions for [preprocessing](https://feature-engine.trainindata.com/en/latest/user_guide/preprocessing/index.html) that can deal with removed rows and features afterwards\n",
    "\n",
    "- [Useful library for feature engineering](https://feature-engine.trainindata.com/en/latest/quickstart/index.html)"
   ],
   "id": "d7e6ae9af4aec729"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Split features into groups based on the data type\n",
    "\n",
    "- Split features for data types (***remember to insert the case where the columns with more than 50% NaN are not included in the splitting at all***)\n",
    "    - Then the pipeline is build to transform the data types\n",
    "    - Based on the previous splitting, apply Imputation methods to all the features. This is done because we don't know if other datasets will have the same null values ripartition\n",
    "    - At this point we need to **drop** the columns not included in the splitting of data types. This because the columns not included will be the ones with a lot of NAs from the start (more than 50%)\n",
    "\n",
    "> *Eventually we could compare the result of this approach with the result of a parallel approach whereby no columns are dropped and the NaNs are all Imputed. Then see how the two models perform*"
   ],
   "id": "f8cc7355c02b8ee7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:51:51.498783Z",
     "start_time": "2024-07-29T15:51:51.487918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## To decide if will be included or not in the pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def drop_features_with_many_nan(x: pd.DataFrame) -> pd.DataFrame:\n",
    "    nulls_summary = pd.DataFrame(df_listings.isnull().sum())\n",
    "    more_than_null_features = nulls_summary.loc[nulls_summary.iloc[:, 0] > df_listings.shape[0]*0.5, :].index.tolist()\n",
    "    return x.drop(more_than_null_features, axis=1)\n",
    "\n",
    "fun_tr_drop_features_with_many_nan = FunctionTransformer(drop_features_with_many_nan)"
   ],
   "id": "c5978ab20ef7796",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define groups for data transformation",
   "id": "4619392d43c4fa28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The following class serves as definition of some general functions to be used for geographic handling",
   "id": "3aaa3fcd554dd608"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:51:53.075963Z",
     "start_time": "2024-07-29T15:51:53.058027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "strategic_locations_geo = {\"Aereoporto Marco Polo\": [45.50354, 12.34258],\n",
    "                       \"Piazza Erminio Ferretto\": [45.49479, 12.24251],\n",
    "                       \"Piazzale Roma\": [45.43801, 12.31885],\n",
    "                       \"Ponte di Rialto\": [45.43805, 12.33593],\n",
    "                       \"Piazza San Marco\": [45.434, 12.338]\n",
    "                       }\n",
    "\n",
    "\n",
    "class GeoDataHandler:\n",
    "    def __init__(self, user_agent: str = \"GeoDataHandler\"):\n",
    "        \"\"\"\n",
    "        Initializes the GeoDataHandler with a user agent for Nominatim.\n",
    "        :param user_agent: A string representing the user agent for Nominatim.\n",
    "        \"\"\"\n",
    "        self.geolocator = Nominatim(user_agent=user_agent)\n",
    "        self.geocode = RateLimiter(self.geolocator.geocode, min_delay_seconds=1.1)\n",
    "    \n",
    "    def retrieve_host_location(self, df: pd.DataFrame) -> dict:\n",
    "        \"\"\"\n",
    "        From a dataset of listings, extracts the list of unique host locations\n",
    "        and retrieve latitude and longitude of every location.\n",
    "        :param df: pandas DataFrame of listings.\n",
    "        :return: dict of locations: [latitude, longitude]\n",
    "        \"\"\"\n",
    "        location_geo = {}\n",
    "        try:\n",
    "            for location in df['host_location'].unique().tolist():\n",
    "                host_location = self.geocode(location)\n",
    "                if host_location:\n",
    "                    location_geo[location] = (host_location.latitude, host_location.longitude)\n",
    "                else:\n",
    "                    location_geo[location] = (None, None)\n",
    "            return location_geo\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    def export_to_json(self, dict_object: dict, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Given a dict with host locations, saves it to a custom path.\n",
    "        :param dict_object: dictionary to be saved as JSON.\n",
    "        :param path: str with the path where to save JSON.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(path, 'w') as f:\n",
    "                json.dump(dict_object, f)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while exporting to JSON: {e}\")\n",
    "\n",
    "    def import_from_json(self, path: str) -> dict:\n",
    "        \"\"\"\n",
    "        Import host location from saved JSON.\n",
    "        :param path: path where the JSON is saved.\n",
    "        :return: JSON in dictionary form.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(path, 'r') as f:\n",
    "                dict_object = json.load(f)\n",
    "            return dict_object\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while importing from JSON: {e}\")\n",
    "            return None\n"
   ],
   "id": "26bfd4171739716f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:51:53.732246Z",
     "start_time": "2024-07-29T15:51:53.632674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "handler = GeoDataHandler()\n",
    "#locations = handler.retrieve_host_location(df_listings)\n",
    "#handler.export_to_json(locations, \"../data/2023dic/host_locations.json\")\n",
    "locations = handler.import_from_json(\"../data/2023dic/host_locations.json\")\n",
    "\n",
    "strategic_locations_geo = {\"Aereoporto Marco Polo\": [45.50354, 12.34258],\n",
    "                       \"Piazza Erminio Ferretto\": [45.49479, 12.24251],\n",
    "                       \"Piazzale Roma\": [45.43801, 12.31885],\n",
    "                       \"Ponte di Rialto\": [45.43805, 12.33593],\n",
    "                       \"Piazza San Marco\": [45.434, 12.338]\n",
    "                       }\n",
    "#handler.export_to_json(strategic_locations_geo, \"../data/strategic_locations.json\")\n",
    "strategic_locations = handler.import_from_json(\"../data/strategic_locations.json\")"
   ],
   "id": "3a3fd1ef73534c2",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Geographical Features",
   "id": "755900984221522c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#df_listings.dtypes\n",
    "df_listings"
   ],
   "id": "a2aec621015ddfa3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:51:58.739252Z",
     "start_time": "2024-07-29T15:51:58.732323Z"
    }
   },
   "cell_type": "code",
   "source": "geo_features = [\"host_location\"]",
   "id": "caf1d6e4ad1b639",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:51:59.701701Z",
     "start_time": "2024-07-29T15:51:59.684840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GeographicTransformer(BaseEstimator, TransformerMixin):\n",
    "    # https://datascience.stackexchange.com/questions/117200/creating-new-features-as-linear-combination-of-others-as-part-of-a-scikit-learn\n",
    "    # https://www.andrewvillazon.com/custom-scikit-learn-transformers/\n",
    "    def __init__(self, host_locations: dict = locations, column: str =\"host_location\"):\n",
    "        \n",
    "        self.column = column\n",
    "        self.host_locations = host_locations\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame, y=None):\n",
    "        X = self.transform_to_coordinates(X, self.host_locations)\n",
    "        X[self.column] = X.apply(lambda row: self.geodesic_distancer(row, from_loc=\"host_location\"), axis=1)\n",
    "        return X\n",
    "    \n",
    "    def transform_to_coordinates(self, X, locations: dict):\n",
    "        \"\"\"\n",
    "        Given an entry and a dictionary, returns the latitude, longitude for\n",
    "        the entry that are saved in the dictionary\n",
    "        :param entry: entry (from dataframe)\n",
    "        :param locations: dict of locations:[latitude, longitude]\n",
    "        :return: [latitude, longitude]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            X[self.column] = X[self.column].apply(lambda x: locations.get(x))\n",
    "            return X\n",
    "        except:\n",
    "            return X\n",
    "        \n",
    "    def geodesic_distancer(self, row, from_loc: str):\n",
    "        try:\n",
    "            coords_1 = (row[from_loc][0], row[from_loc][1])\n",
    "            coords_2 = (row[\"latitude\"], row[\"longitude\"])\n",
    "            return geodesic(coords_1, coords_2).km\n",
    "        except:\n",
    "            return None\n",
    "        "
   ],
   "id": "f6f9b580f4501783",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:52:00.474474Z",
     "start_time": "2024-07-29T15:52:00.468637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "geographic_pipeline = Pipeline(steps=[\n",
    "    ('Host location transformer', GeographicTransformer(column=\"host_location\", host_locations=locations))\n",
    "])"
   ],
   "id": "bc8f2c1aeb6c4e9c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### String features",
   "id": "75cdfcb5a9dfbcdf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:52:07.072043Z",
     "start_time": "2024-07-29T15:52:07.064515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "string_features = [\"neighborhood_overview\",\n",
    "                   \"host_about\"]"
   ],
   "id": "3f2f6b7237a34b54",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Procedure for the string features in order to extract encoded features from text:\n",
    "- use the tf-idf in order to gain a vector of encoded normalized word scores\n",
    "- Use the vector as a feature in the dataset\n",
    "- the vector does not need other normalization aspects"
   ],
   "id": "45fa1ed0d4d0c623"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:52:09.110777Z",
     "start_time": "2024-07-29T15:52:09.103020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def trasform_nan_unicode(text_series):\n",
    "    return text_series.fillna(\"\").astype('U')\n",
    "\n",
    "text_encoding_pipeline = Pipeline(steps=[\n",
    "    (\"text preprocessing\", FunctionTransformer(trasform_nan_unicode, validate=False)),\n",
    "    (\"tf-idf vectorizer\", TfidfVectorizer(encoding='utf-8',\n",
    "                                          decode_error='ignore',\n",
    "                                          strip_accents='unicode',\n",
    "                                          lowercase=True,\n",
    "                                          analyzer='word',\n",
    "                                          max_df=0.8,\n",
    "                                          use_idf=True,\n",
    "                                          smooth_idf=True)\n",
    "     )\n",
    "])"
   ],
   "id": "b70d1a23bcf37aa7",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Rates features",
   "id": "c8a69e0ca2efac17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:52:11.941656Z",
     "start_time": "2024-07-29T15:52:11.934682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rate_feature = [\"host_response_rate\",\n",
    "                \"host_acceptance_rate\"]"
   ],
   "id": "222bdedf65f1d7ba",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:52:12.717422Z",
     "start_time": "2024-07-29T15:52:12.710870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def from_string_to_rate(rate_string: str) -> float:\n",
    "    return rate_string.str.rstrip('%').astype(float)\n",
    "    "
   ],
   "id": "82fb53c30febd6af",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:52:13.592902Z",
     "start_time": "2024-07-29T15:52:13.587285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rates_pipeline = Pipeline(steps=[\n",
    "    (\"Transform response rate\", FunctionTransformer(from_string_to_rate))\n",
    "])"
   ],
   "id": "7d85c87b7799146",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Time features",
   "id": "94ef10fa86378680"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:52:15.662536Z",
     "start_time": "2024-07-29T15:52:15.655903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "time_feature = [\"host_since\",\n",
    "                \"first_review\",\n",
    "                \"last_review\"]"
   ],
   "id": "561c1cfef5706295",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:52:16.560502Z",
     "start_time": "2024-07-29T15:52:16.554284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def trasform_to_datetime(text_date: str) -> pd.Timestamp | pd.Timestamp:\n",
    "    return pd.to_datetime(text_date)"
   ],
   "id": "2064243695abe512",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:52:17.459582Z",
     "start_time": "2024-07-29T15:52:17.453872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "timestamp_pipeline = Pipeline(steps=[\n",
    "    (\"Trasform to timestamp\", FunctionTransformer(trasform_to_datetime))\n",
    "])"
   ],
   "id": "72fcd46543902930",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Numerical features",
   "id": "e9e054fc6211afeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_features = [\"host_listings_count\", \"host_total_listings_count\", \"accommodates\", \"bathrooms\", \"bedrooms\", \"beds\",\n",
    "                \"minimum_nights\", \"maximum_nights\", \"availability_30\", \"availability_60\", \"availability_90\",\n",
    "                \"availability_365\", \"number_of_reviews\", \"review_scores_rating\", \"review_scores_accuracy\",\n",
    "                \"review_scores_cleanliness\", \"review_scores_checkin\", \"review_scores_communication\",\n",
    "                \"review_scores_location\", \"review_scores_value\", \"calculated_host_listings_count\",\n",
    "                \"calculated_host_listings_count_entire_homes\", \"calculated_host_listings_count_private_rooms\",\n",
    "                \"calculated_host_listings_count_shared_rooms\", \"reviews_per_month\"\n",
    "                ]"
   ],
   "id": "4a5eec912159bebd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "88e10d3761bd4af3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def transform_to_date(date) -> pd.DatetimeTZDtype:\n",
    "    try:\n",
    "        return pd.to_datetime(date)\n",
    "    except:\n",
    "        return None\n",
    "        \n",
    "\n",
    "\n"
   ],
   "id": "6e8ed07572ab5f28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "342313067519749c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Add and manipulate features",
   "id": "79720a1c16896a6a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
